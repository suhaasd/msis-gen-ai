# Gen AI Security

### 2025 Top 10 Risk & Mitigations for LLMs and Gen AI Apps
- https://genai.owasp.org/llm-top-10/

## LLM Guardrails

### How to implement LLM guardrails
- https://cookbook.openai.com/examples/how_to_use_guardrails

### Guardrails AI - Guardrails Project
Guardrails is a Python framework that helps build reliable AI applications by performing two key functions:

Guardrails runs Input/Output Guards in your application that detect, quantify and mitigate the presence of specific types of risks. To look at the full suite of risks, check out Guardrails Hub.
Guardrails help you generate structured data from LLMs.

- https://github.com/guardrails-ai/guardrails


### New prompt injection papers: Agents Rule of Two and The Attacker Moves Second

- https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/
- Simon Willison’s Weblog


### Agents Rule of Two: A Practical Approach to AI Agent Security
Meta's most recent blog post about AI security.
Imagine a personal AI agent, Email-Bot, that’s designed to help you manage your inbox. In order to provide value and operate effectively, Email-Bot might need to:

Access unread email contents from various senders to provide helpful summaries
Read through your existing email inbox to keep track of any important updates, reminders, or context
Send replies or follow-up emails on your behalf
While the automated email assistant can be of great help, this hypothetical bot can also demonstrate how AI agents are introducing novel risks. Notably, one of the biggest challenges for the industry is that of agents’ susceptibility to prompt injection.

- https://ai.meta.com/blog/practical-ai-agent-security/
- October 31, 2025



### THE ATTACKER MOVES SECOND: STRONGER ADAPTIVE ATTACKS BYPASS DEFENSES AGAINST LLM JAILBREAKS AND PROMPT INJECTIONS
- https://arxiv.org/pdf/2510.09023

